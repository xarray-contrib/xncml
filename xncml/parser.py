"""
# NcML parser for xarray

The `open_ncml` function parse an XML document compliant with the NcML-2.2 schema and returns an xarray Dataset.

The XML is parsed into a Python objects using `xsdata`. The parser converts XML elements into classes instances
defined in an autogenerated data model (`generated.ncml_2_2`). This datamodel was created using:

  ```bash
  xsdata generate -ds NumPy --compound-fields -mll=119 --postponed-annotations  schemas/ncml-2.2.xsd
  ```

The code below converts the NcML instructions into xarray instructions. Not all NcML instructions are currently
supported.


## TODO

Support for these elements is missing:
- <cacheVariable>
- <logicalReduce>
- <logicalSection>
- <logicalSlice>
- <promoteGlobalAttribute>
- <enumTypedef>
- <group>
- <scanFmrc>

Support for these attributes is missing:
- dateFormatMark
- olderThan
- tiled aggregations





"""
__author__ = "David Huard"
__date__ = "July 2022"
__contact__ = "huard.david@ouranos.ca"

from .generated import (
    Netcdf,
    DataType,
    Aggregation,
    Dimension,
    Attribute,
    Variable,
    AggregationType,
    Remove,
    ObjectType,
    Values,
    Group,
    EnumTypedef
)
from xsdata.formats.dataclass.parsers import XmlParser
import numpy as np
from pathlib import Path
import xarray as xr
import datetime as dt


def parse(path: Path) -> Netcdf:
    """
    Parse NcML file using NetCDF datamodel based on NcML-2.2 Schema.

    Parameters
    ----------
    path : Path
      Path to NcML file.

    Returns
    -------
    Netcdf instance.
      Object description of NcML content.
    """
    parser = XmlParser()
    return parser.from_path(path, Netcdf)


def open_ncml(ncml: str | Path) -> xr.Dataset:
    """
    Convert NcML document to a dataset.

    Parameters
    ----------
    ncml : str | Path
      Path to NcML file.

    Returns
    -------
    xr.Dataset
      Dataset holding variables and attributes defined in NcML document.
    """
    # Parse NcML document
    ncml = Path(ncml)
    obj = parse(ncml)

    # Open location if any
    ref = read_ds(obj, ncml) or xr.Dataset()
    target = xr.Dataset()

    # <explicit/> element means that only content specifically mentioned in NcML document is included in dataset.
    if obj.read_metadata is not None:
        target = ref
    elif obj.explicit is not None:
        pass
    else:  # Default
        target = ref

    return read_netcdf(target, ref, obj, ncml)


def read_netcdf(target: xr.Dataset, ref: xr.Dataset, obj: Netcdf, ncml: Path) -> xr.Dataset:
    """
    Return content of <netcdf> element.

    Parameters
    ----------
    target : xr.Dataset
      Target dataset to be updated with <netcdf>'s content.
    ref : xr.Dataset
      Reference dataset used to copy content into `target`.
    obj : Netcdf
       <netcdf> object description.
    ncml : Path
      Path to NcML document, sometimes required to follow relative links.

    Returns
    -------
    xr.Dataset
      Dataset holding variables and attributes defined in <netcdf> element.
    """
    # We need to start parsing <aggregation>, since it contains required information for later <variable> elements.
    for item in filter_by_class(obj.choice, Aggregation):
        target = read_aggregation(target, item, ncml)

    target = read_group(target, ref, obj)

    return target


def read_aggregation(target: xr.Dataset, obj: Aggregation, ncml: Path) -> xr.Dataset:
    """
    Return merged or concatenated content of <aggregation> element.

    Parameters
    ----------
    target : xr.Dataset
      Target dataset to be updated with <netcdf>'s content.
    obj : Aggregation
       <aggregation> object description.
    ncml : Path
      Path to NcML document, sometimes required to follow relative links.

    Returns
    -------
    xr.Dataset
      Dataset holding variables and attributes defined in <aggregation> element.
    """
    from xarray.coding.times import CFDatetimeCoder

    # Names of variables to be aggregated. All variables if undefined.
    names = [v.name for v in filter_by_class(obj.choice, Aggregation.VariableAgg)]

    # Create list of items to aggregate.
    items = []
    for item in obj.netcdf:
        # Open dataset defined in <netcdf>'s `location` attribute
        ref = read_ds(item, ncml) or xr.Dataset()

        # Handle <explicit/> tag.
        tar = xr.Dataset()
        if item.read_metadata is not None:
            tar = ref
        elif item.explicit is not None:
            pass
        else:  # Default
            tar = ref

        # Handle <variable>, <attribute> and <remove> elements
        tar = read_group(tar, ref, item)

        # Select variables
        if names:
            tar = tar[names]

        # Handle coordinate values
        if item.coord_value is not None:
            dtypes = [i[obj.dim_name].dtype.type for i in [tar, target, ref] if obj.dim_name in i]
            coords = read_coord_value(item, obj, dtypes=dtypes)
            tar = tar.assign_coords({obj.dim_name: coords})
        items.append(tar)

    # Handle <scan> element
    for item in obj.scan:
        items.extend(read_scan(item, ncml))

    # Need to decode time variable
    if obj.time_units_change:
        for i, ds in enumerate(items):
            t = xr.as_variable(ds[obj.dim_name], obj.dim_name)  # Maybe not the same name...
            encoded = CFDatetimeCoder(use_cftime=True).decode(t, name=t.name)
            items[i] = ds.assign_coords({obj.dim_name: encoded})

    # Translate different types of aggregation into xarray instructions.
    match obj.type:
        case AggregationType.JOIN_EXISTING:
            agg = xr.concat(items, obj.dim_name)
        case AggregationType.JOIN_NEW:
            agg = xr.concat(items, obj.dim_name)
        case AggregationType.UNION:
            agg = xr.merge(items)
        case _:
            raise NotImplementedError

    return target.merge(agg)


def read_ds(obj: Netcdf, ncml: Path) -> xr.Dataset:
    """
    Return dataset defined in <netcdf> element.

    Parameters
    ----------
    obj : Netcdf
      <netcdf> object description.
    ncml : Path
      Path to NcML document, sometimes required to follow relative links.

    Returns
    -------
    xr.Dataset
      Dataset defined at <netcdf>' `location` attribute.
    """
    if obj.location:
        location = obj.location.removeprefix("file:")
        if not Path(location).is_absolute():
            location = ncml.parent / location
        return xr.open_dataset(location, decode_times=False)


def read_group(target: xr.Dataset, ref: xr.Dataset, obj: Group | Netcdf) -> xr.Dataset:
    """
    Parse <group> items, typically <dimension>, <variable>, <attribute> and <remove> elements.

    Parameters
    ----------
    target : xr.Dataset
      Target dataset to be updated.
    ref : xr.Dataset
      Reference dataset used to copy content into `target`.
    obj : Group | Netcdf
       <netcdf> object description.

    Returns
    -------
    xr.Dataset
      Dataset holding variables and attributes defined in <netcdf> element.
    """
    dims = {}
    for item in obj.choice:
        match item:
            case Dimension():
                dims[item.name] = read_dimension(item)
            case Variable():
                target = read_variable(target, ref, item, dims)
            case Attribute():
                read_attribute(target, item, ref)
            case Remove():
                target = read_remove(target, item)
            case EnumTypedef():
                raise NotImplementedError
            case Group():
                target = read_group(target, ref, item)
            case Aggregation():
                pass  # <aggregation> elements are parsed in `read_netcdf`
            case _:
                raise AttributeError

    return target


def read_scan(obj: Aggregation.Scan, ncml: Path) -> [xr.Dataset]:
    """
    Return list of datasets defined in <scan> element.

    Parameters
    ----------
    obj : Aggregation.Scan instance
      <scan> object description.
    ncml : Path
      Path to NcML document, sometimes required to follow relative links.

    Returns
    -------
    list
      List of datasets found by scan.
    """
    import re

    if obj.date_format_mark:
        raise NotImplementedError

    path = Path(obj.location)
    if not path.is_absolute():
        path = ncml.parent / path

    files = path.rglob("*") if obj.subdirs else path.glob("*")

    if obj.reg_exp:
        pat = re.compile(obj.reg_exp)
        files = filter(pat.match, map(str, files))
    elif obj.suffix:
        pat = "*" + obj.suffix
        files = [f for f in files if f.match(pat)]

    files.sort()

    return [xr.open_dataset(f, decode_times=False) for f in files]


def read_coord_value(nc: Netcdf, agg: Aggregation, dtypes: list = ()):
    """
    Read `coordValue` attribute of <netcdf> element.

    Parameters
    ----------
    nc : Netcdf instance
      <netcdf> object description.
    agg : Aggregation instance
      <aggregation> object description
    dtypes : tuple
      List of preferred type for coordinate value.

    Returns
    -------
    str, np.array, scalar
      Coordinate value cast to proper type.

    Notes
    -----
    The casting logic is most likely not up to spec.
    """
    val = nc.coord_value

    match agg.type:
        # A JOIN_NEW aggregation has exactly one coordinate value
        case AggregationType.JOIN_NEW:
            coord = val
        case AggregationType.JOIN_EXISTING:
            if "," in val:
                coord = val.split(",")
            elif " " in val:
                coord = val.split(" ")
            else:
                coord = [val]

        case _:
            raise NotImplementedError

    # Cast to dtype, not clear what the spec is exactly for this.
    if dtypes:
        typ = dtypes[0]
    else:
        try:
            dt.datetime.strptime(coord, "%Y-%m-%d %H:%M:%SZ")
            typ = str
        except ValueError:
            typ = np.float64

    return typ(coord)


def read_variable(target: xr.Dataset, ref: xr.Dataset, obj: Variable, dimensions: dict):
    """
    Parse <variable> element.

    Parameters
    ----------
    target : xr.Dataset
      Target dataset to be updated.
    ref : xr.Dataset
      Reference dataset used to copy content into `target`.
    obj : Variable
       <variable> object description.
    dimensions : dict
      Dimension attributes keyed by name.

    Returns
    -------
    xr.Dataset
      Dataset holding variable defined in <variable> element.
    """
    # Handle logic for variable name change
    if obj.org_name:
        if obj.org_name in target:
            target = target.rename({obj.org_name: obj.name})
            ref_var = None
        elif obj.org_name in ref:
            ref_var = xr.as_variable(ref[obj.org_name])
        else:
            raise ValueError
    else:
        ref_var = None

    # Read existing data or create empty DataArray
    if (obj.name in target) or (obj.name in target.dims):
        out = xr.as_variable(target[obj.name])
        if obj.type:
            out = out.astype(nctype(obj))
        ref_var = None
    elif (obj.name in ref) or (obj.name in ref.dims):
        out = xr.as_variable(ref[obj.name])
        if obj.type:
            out = out.astype(nctype(obj))
        ref_var = ref[obj.name]
    elif obj.shape:
        dims = obj.shape.split(" ")
        shape = [dimensions[dim].length for dim in dims]
        out = xr.Variable(
            data=np.empty(shape, dtype=nctype(obj)), dims=dims)
    else:
        raise ValueError

    # Set variable attributes
    for item in obj.attribute:
        read_attribute(out, item, ref=ref_var)

    # Remove attributes or dimensions
    for item in obj.remove:
        read_remove(out, item)

    # Read values
    if obj.values:
        out = read_values(out, obj.values)

    if obj.logical_section:
        raise NotImplementedError

    if obj.logical_slice:
        raise NotImplementedError

    if obj.logical_reduce:
        raise NotImplementedError

    if obj.typedef:
        raise NotImplementedError

    target[obj.name] = out
    return target


def read_values(v: xr.Variable, obj: Values) -> xr.Variable:
    """Read values for <variable> element.

    Parameters
    ----------
    v : xr.DataArray
      Array whose values are to be updated.
    obj : Values instance
      <values> object description

    Returns
    -------
    xr.Variable
      Array filled with values from <values> element.
    """
    if obj.from_attribute is not None:
        raise NotImplementedError

    n = int(obj.npts or v.size)
    if obj.start is not None and obj.increment is not None:
        data = obj.start + np.arange(n) * obj.increment
    else:
        sep = obj.separator or " "
        if isinstance(obj.content, list) and isinstance(obj.content[0], str):
            data = obj.content[0].split(sep)
        else:
            raise NotImplementedError

    data = v.dtype.type(data)
    return xr.Variable(v.dims, data, v.attrs)


def read_remove(target: xr.Dataset | xr.Variable, obj: Remove) -> xr.Dataset:
    """Remove item from dataset.

    Parameters
    ----------
    target : xr.Dataset | xr.Variable
      Target dataset or variable to be updated.
    obj : Remove instance
      <remove> object description.

    Returns
    -------
    xr.Dataset or xr.Variable
      Dataset with attribute, variable or dimension removed, or variable with attribute removed.
    """
    match obj.type:
        case ObjectType.ATTRIBUTE:
            target.attrs.pop(obj.name)
        case ObjectType.VARIABLE:
            target = target.drop_vars(obj.name)
        case ObjectType.DIMENSION:
            target = target.drop_dims(obj.name)

    return target


def read_attribute(target: xr.Dataset | xr.Variable, obj: Attribute, ref: xr.Dataset = None):
    """Update target dataset in place with new or modified attribute.

    Parameters
    ----------
    target : xr.Dataset | xr.Variable
      Target dataset to be updated.
    obj : Attribute instance
      <attribute> object description.
    ref : xr.Dataset
      Reference dataset.
    """
    if obj.value is not None:
        target.attrs[obj.name] = cast(obj)
    elif obj.org_name is not None:
        try:
            target.attrs[obj.name] = ref.attrs[obj.org_name]
        except (AttributeError, KeyError):
            target.attrs[obj.name] = target.attrs.get(obj.org_name)
        target.attrs.pop(obj.org_name)
    elif ref is not None:
        target.attrs[obj.name] = ref.attrs.get(obj.name)
    else:
        raise NotImplementedError


def read_dimension(obj: Dimension) -> Dimension:
    """Return dimension object with its length cast to an integer."""
    obj.length = int(obj.length)
    return obj


def nctype(obj: [Attribute, Variable]) -> type:
    """Return Python type corresponding to the NcML DataType of object."""
    match obj.type:
        case DataType.STRING | DataType.STRING_1:
            return str
        case DataType.BYTE:
            return np.int8
        case DataType.SHORT:
            return np.int16
        case DataType.INT:
            return np.int32
        case DataType.LONG:
            return int
        case DataType.FLOAT:
            return np.float32
        case DataType.DOUBLE:
            return np.float64
        case _:
            raise NotImplementedError


def cast(obj: Attribute):
    """Cast attribute value to the appropriate type."""
    value = obj.value or obj.content
    if value:
        if obj.type in [DataType.STRING, DataType.STRING_1]:
            return value

        sep = obj.separator or " "
        values = value.split(sep)
        return tuple(map(nctype(obj), values))

        return nctype(obj)(value)


def filter_by_class(iterable, klass):
    """Return generator filtering on class."""
    for item in iterable:
        if isinstance(item, klass):
            yield item
